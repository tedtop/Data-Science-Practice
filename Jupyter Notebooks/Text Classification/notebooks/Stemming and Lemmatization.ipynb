{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\javier.perez-\n",
      "[nltk_data]     alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\javier.perez-\n",
      "[nltk_data]     alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\javier.perez-\n",
      "[nltk_data]     alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\javier.perez-\n",
      "[nltk_data]     alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\javier.perez-\n",
      "[nltk_data]     alvaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import NLTK (natural language toolkit)\n",
    "import nltk \n",
    "nltk.download('wordnet') # \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4') # open multilingual wordnet library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization are techniques to nomalize text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading -> read\n",
    "\n",
    "Books -> book\n",
    "\n",
    "Stories -> stori (from stemming) or story (from lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More info [here](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem with the bag-of-words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy training data\n",
    "X_train = ['I love the book',\n",
    "           'This is a great book',\n",
    "           'The fit is great',\n",
    "           'I love the shoes']\n",
    "y_train = ['books',\n",
    "           'books',\n",
    "           'clothings',\n",
    "           'clothings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>fit</th>\n",
       "      <th>great</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>shoes</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I love the book</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This is a great book</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The fit is great</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I love the shoes</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      book  fit  great  is  love  shoes  the  this\n",
       "I love the book          1    0      0   0     1      0    1     0\n",
       "This is a great book     1    0      1   1     0      0    0     1\n",
       "The fit is great         0    1      1   1     0      0    1     0\n",
       "I love the shoes         0    0      0   0     1      1    1     0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "pd.DataFrame(data=X_train_dtm.toarray(),\n",
    "             columns=vect.get_feature_names_out(),\n",
    "             index=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a naive bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X_train_dtm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['books', 'clothings', 'clothings', 'books'], dtype='<U9')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toy testing data \n",
    "X_test = ['I like the book',\n",
    "          'Shoes are alright',\n",
    "          'I love the books',\n",
    "          'I lost a shoe']\n",
    "\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "nb_clf.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions for 'I love the books' and 'I lost a shoe' are wrong. Why? Because the model hasn't seen the words 'books' and 'shoe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the stemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('reading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'organ'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# organize, organizes, and organizing\n",
    "stemmer.stem('organize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'organ'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('organizes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'organ'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('organizing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer breaks a sentence into its individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'the', 'books', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = 'I love the books.'\n",
    "words = word_tokenize(phrase)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'the', 'book', '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love the book .'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lemmatizer expects the parts of speech; by default, each token is a noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('eats', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('ate', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('running', pos='v') # she is running "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('running', pos='n') # running is good for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('better', pos = 'r')  # She sings better than me (adverb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'better'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('better', pos='v') # to better oneself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('better', pos='a') # The better team won the match (adjective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'), ('love', 'VBP'), ('the', 'DT'), ('books', 'NNS'), ('.', '.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parts of speech tagging\n",
    "pos_list = nltk.pos_tag(words)\n",
    "pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process parts of speech function\n",
    "def process_pos(pos):\n",
    "    if pos.startswith('J'): # adjectives\n",
    "        return wordnet.ADJ\n",
    "    elif pos.startswith('V'): # verbes\n",
    "        return wordnet.VERB\n",
    "    elif pos.startswith('N'): # nouns\n",
    "        return wordnet.NOUN\n",
    "    elif pos.startswith('R'): # adverbs\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'the', 'books', '.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'the', 'book', '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words = [lemmatizer.lemmatize(word, pos=process_pos(pos)) \n",
    "                    for word,pos \n",
    "                    in nltk.pos_tag(words)]\n",
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love the book .'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of most common words in english: this, that, he, it, ... They don't add much meaning to the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here is an example sentence demostrating the removal of stopwords'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = 'here is an example sentence demostrating the removal of stopwords'\n",
    "phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'example sentence demostrating removal stopwords'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(phrase)\n",
    "stripped_phrase = [word for word in words if word not in stop_words]\n",
    "\" \".join(stripped_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punctuation = [punc for punc in string.punctuation]\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello How are you'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = 'Hello! How are you?'\n",
    "words = word_tokenize(phrase)\n",
    "stripped_phrase = [word for word in words if word not in punctuation]\n",
    "\" \".join(stripped_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  stars\n",
       "0  My wife took me here on my birthday for breakf...      5\n",
       "1  I have no idea why some people give bad review...      5\n",
       "2  love the gyro plate. Rice is so good and I als...      4\n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...      5\n",
       "4  General Manager Scott Petello is a good egg!!!...      5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/um-perez-alvaro/Data-Science-Practice/master/Data/yelp.csv'\n",
    "yelp = pd.read_csv(url)[['text','stars']]\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       5\n",
       "1       5\n",
       "2       4\n",
       "3       5\n",
       "4       5\n",
       "       ..\n",
       "9995    3\n",
       "9996    4\n",
       "9997    4\n",
       "9998    2\n",
       "9999    5\n",
       "Name: stars, Length: 10000, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Drop what you're doing and drive here. After I...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4081</th>\n",
       "      <td>Yes I do rock the hipster joints.  I dig this ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4082</th>\n",
       "      <td>Only 4 stars? \\n\\n(A few notes: The folks that...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>I'm not normally one to jump at reviewing a ch...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4084</th>\n",
       "      <td>Let's see...what is there NOT to like about Su...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085</th>\n",
       "      <td>4-5 locations.. all 4.5 star average.. I think...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4086 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  stars\n",
       "0     My wife took me here on my birthday for breakf...      5\n",
       "1     I have no idea why some people give bad review...      5\n",
       "2     Rosie, Dakota, and I LOVE Chaparral Dog Park!!...      5\n",
       "3     General Manager Scott Petello is a good egg!!!...      5\n",
       "4     Drop what you're doing and drive here. After I...      5\n",
       "...                                                 ...    ...\n",
       "4081  Yes I do rock the hipster joints.  I dig this ...      5\n",
       "4082  Only 4 stars? \\n\\n(A few notes: The folks that...      5\n",
       "4083  I'm not normally one to jump at reviewing a ch...      5\n",
       "4084  Let's see...what is there NOT to like about Su...      5\n",
       "4085  4-5 locations.. all 4.5 star average.. I think...      5\n",
       "\n",
       "[4086 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep reviews that only contains the 5-stars and 1-star reviews\n",
    "yelp = yelp[yelp.stars.isin([1,5])].reset_index(drop=True)\n",
    "yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n",
      "\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n",
      "\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n",
      "\n",
      "Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "print(yelp.loc[0,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wife take birthday breakfast excellent weather perfect make sit outside overlook ground absolute pleasure waitress excellent food arrive quickly semi-busy saturday morning look like place fill pretty quickly early get good favor get bloody mary phenomenal simply best 've ever 'm pretty sure use ingredient garden blend fresh order amaze everything menu look excellent white truffle scramble egg vegetable skillet tasty delicious come 2 piece griddle bread amaze absolutely make meal complete best `` toast '' 've ever anyway ca n't wait go back\n"
     ]
    }
   ],
   "source": [
    "text = yelp.loc[0,'text']\n",
    "words = word_tokenize(text)\n",
    "words = [word.lower() for word in words]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos=process_pos(pos)) \n",
    "                    for word,pos in nltk.pos_tag(words) \n",
    "                    if word not in stop_words and word not in punctuation]\n",
    "print(' '.join(lemmatized_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(yelp)):\n",
    "    text = yelp.loc[i,'text']\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos=process_pos(pos)) \n",
    "                        for word,pos in nltk.pos_tag(words) \n",
    "                        if word not in stop_words and word not in punctuation]\n",
    "    yelp.loc[i,'processed_text'] = ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>5</td>\n",
       "      <td>wife take birthday breakfast excellent weather...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>5</td>\n",
       "      <td>idea people give bad review place go show plea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>5</td>\n",
       "      <td>rosie dakota love chaparral dog park 's conven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>5</td>\n",
       "      <td>general manager scott petello good egg go deta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Drop what you're doing and drive here. After I...</td>\n",
       "      <td>5</td>\n",
       "      <td>drop 're drive eat go back next day food good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4081</th>\n",
       "      <td>Yes I do rock the hipster joints.  I dig this ...</td>\n",
       "      <td>5</td>\n",
       "      <td>yes rock hipster joint dig place little bit sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4082</th>\n",
       "      <td>Only 4 stars? \\n\\n(A few notes: The folks that...</td>\n",
       "      <td>5</td>\n",
       "      <td>4 star note folk rat place low must isolate in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>I'm not normally one to jump at reviewing a ch...</td>\n",
       "      <td>5</td>\n",
       "      <td>'m normally one jump review chain restaurant e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4084</th>\n",
       "      <td>Let's see...what is there NOT to like about Su...</td>\n",
       "      <td>5</td>\n",
       "      <td>let 's see ... like surprise stadium well 9.50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085</th>\n",
       "      <td>4-5 locations.. all 4.5 star average.. I think...</td>\n",
       "      <td>5</td>\n",
       "      <td>4-5 location .. 4.5 star average .. think ariz...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4086 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  stars  \\\n",
       "0     My wife took me here on my birthday for breakf...      5   \n",
       "1     I have no idea why some people give bad review...      5   \n",
       "2     Rosie, Dakota, and I LOVE Chaparral Dog Park!!...      5   \n",
       "3     General Manager Scott Petello is a good egg!!!...      5   \n",
       "4     Drop what you're doing and drive here. After I...      5   \n",
       "...                                                 ...    ...   \n",
       "4081  Yes I do rock the hipster joints.  I dig this ...      5   \n",
       "4082  Only 4 stars? \\n\\n(A few notes: The folks that...      5   \n",
       "4083  I'm not normally one to jump at reviewing a ch...      5   \n",
       "4084  Let's see...what is there NOT to like about Su...      5   \n",
       "4085  4-5 locations.. all 4.5 star average.. I think...      5   \n",
       "\n",
       "                                         processed_text  \n",
       "0     wife take birthday breakfast excellent weather...  \n",
       "1     idea people give bad review place go show plea...  \n",
       "2     rosie dakota love chaparral dog park 's conven...  \n",
       "3     general manager scott petello good egg go deta...  \n",
       "4     drop 're drive eat go back next day food good ...  \n",
       "...                                                 ...  \n",
       "4081  yes rock hipster joint dig place little bit sc...  \n",
       "4082  4 star note folk rat place low must isolate in...  \n",
       "4083  'm normally one jump review chain restaurant e...  \n",
       "4084  let 's see ... like surprise stadium well 9.50...  \n",
       "4085  4-5 location .. 4.5 star average .. think ariz...  \n",
       "\n",
       "[4086 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = yelp.stars\n",
    "X = yelp.processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('vect', CountVectorizer()), \n",
    "    ('clf', MultinomialNB()) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3, 177],\n",
       "       [  0, 842]], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect', CountVectorizer()),\n",
       "                                       ('clf', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'vect__max_df': [1.0, 0.9, 0.8],\n",
       "                         'vect__max_features': [2000, 5000, 7000, 10000],\n",
       "                         'vect__min_df': [5, 25, 50],\n",
       "                         'vect__ngram_range': [(1, 1), (1, 2)]},\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_dic =  {'vect__max_features' : [2000,5000,7000,10000],\n",
    "               'vect__min_df' : [5,25,50],\n",
    "               'vect__max_df' : [1.0,0.9,0.8],\n",
    "               'vect__ngram_range' : [(1,1), (1,2)],\n",
    "               }\n",
    "\n",
    "grid = GridSearchCV(pipe,\n",
    "                    params_dic,\n",
    "                    scoring='accuracy', \n",
    "                    cv=5, \n",
    "                    n_jobs=-1,\n",
    "                    verbose=2)\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vect__max_df': 1.0,\n",
       " 'vect__max_features': 5000,\n",
       " 'vect__min_df': 5,\n",
       " 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipe = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[140,  40],\n",
       "       [ 36, 806]], dtype=int64)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = best_pipe.predict(X_test)\n",
    "confusion_matrix(y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9256360078277887"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the model choose between 5-stars or 1-star ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the vocabulary of X_train\n",
    "words = best_pipe['vect'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5], dtype=int64)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pipe['clf'].classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of times each word appears across all 1-star docs\n",
    "bad_word_count = best_pipe['clf'].feature_count_[0,:]\n",
    "# number of times each word appears across all 5-stars docs\n",
    "good_word_count = best_pipe['clf'].feature_count_[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>27.0</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00pm</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>73.0</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10 15</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bad   good\n",
       "word              \n",
       "00     27.0   35.0\n",
       "000     4.0   10.0\n",
       "00pm    1.0    5.0\n",
       "10     73.0  132.0\n",
       "10 15   2.0    6.0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a DataFrame of words with their separate 1-star and 5-stars counts\n",
    "words = pd.DataFrame({'word' : words,\n",
    "                      'bad' : bad_word_count, \n",
    "                      'good' : good_word_count}).set_index('word')\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 1 to the columns counts to avoid dividing by 0\n",
    "words.bad = words.bad+1\n",
    "words.good = words.good+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.000231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00pm</th>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.000855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10 15</th>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            bad      good\n",
       "word                     \n",
       "00     0.000558  0.000231\n",
       "000    0.000100  0.000071\n",
       "00pm   0.000040  0.000039\n",
       "10     0.001475  0.000855\n",
       "10 15  0.000060  0.000045"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the counts into frequencies\n",
    "words.bad = words.bad/words.bad.sum()\n",
    "words.good = words.good/words.good.sum()\n",
    "words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratios\n",
    "words['bad_ratio'] = words.bad/words.good\n",
    "words['good_ratio'] = words.good/words.bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>bad_ratio</th>\n",
       "      <th>good_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fantastic</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.030255</td>\n",
       "      <td>33.052909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one favorite</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.045604</td>\n",
       "      <td>21.927783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perfect</th>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>0.052561</td>\n",
       "      <td>19.025577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yum</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.056383</td>\n",
       "      <td>17.735707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>favorite</th>\n",
       "      <td>0.00016</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>0.066511</td>\n",
       "      <td>15.035043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amaze</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.072796</td>\n",
       "      <td>13.737111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pasty</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.073835</td>\n",
       "      <td>13.543631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reasonably</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.073835</td>\n",
       "      <td>13.543631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awesome</th>\n",
       "      <td>0.00014</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.075374</td>\n",
       "      <td>13.267230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perfection</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.075636</td>\n",
       "      <td>13.221163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reasonably price</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.077527</td>\n",
       "      <td>12.898696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gem</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.081608</td>\n",
       "      <td>12.253761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bianco</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.086141</td>\n",
       "      <td>11.608826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mozzarella</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>0.088603</td>\n",
       "      <td>11.286359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fabulous</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.089887</td>\n",
       "      <td>11.125125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grilled</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.091208</td>\n",
       "      <td>10.963892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pastry</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.093972</td>\n",
       "      <td>10.641424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one best</th>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.095418</td>\n",
       "      <td>10.480191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ny</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.096909</td>\n",
       "      <td>10.318957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dr</th>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.097929</td>\n",
       "      <td>10.211468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      bad      good  bad_ratio  good_ratio\n",
       "word                                                      \n",
       "fantastic         0.00004  0.001318   0.030255   33.052909\n",
       "one favorite      0.00002  0.000437   0.045604   21.927783\n",
       "perfect           0.00008  0.001517   0.052561   19.025577\n",
       "yum               0.00002  0.000354   0.056383   17.735707\n",
       "favorite          0.00016  0.002398   0.066511   15.035043\n",
       "amaze             0.00010  0.001369   0.072796   13.737111\n",
       "pasty             0.00002  0.000270   0.073835   13.543631\n",
       "reasonably        0.00002  0.000270   0.073835   13.543631\n",
       "awesome           0.00014  0.001852   0.075374   13.267230\n",
       "perfection        0.00002  0.000264   0.075636   13.221163\n",
       "reasonably price  0.00002  0.000257   0.077527   12.898696\n",
       "gem               0.00002  0.000244   0.081608   12.253761\n",
       "bianco            0.00002  0.000231   0.086141   11.608826\n",
       "mozzarella        0.00002  0.000225   0.088603   11.286359\n",
       "fabulous          0.00004  0.000444   0.089887   11.125125\n",
       "grilled           0.00002  0.000219   0.091208   10.963892\n",
       "pastry            0.00002  0.000212   0.093972   10.641424\n",
       "one best          0.00004  0.000418   0.095418   10.480191\n",
       "ny                0.00002  0.000206   0.096909   10.318957\n",
       "dr                0.00006  0.000611   0.097929   10.211468"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.sort_values(by='good_ratio', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>good</th>\n",
       "      <th>bad_ratio</th>\n",
       "      <th>good_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>never return</th>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>37.213064</td>\n",
       "      <td>0.026872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service horrible</th>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>37.213064</td>\n",
       "      <td>0.026872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poor service</th>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>37.213064</td>\n",
       "      <td>0.026872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never come</th>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>34.111975</td>\n",
       "      <td>0.029315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unprofessional</th>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>31.010886</td>\n",
       "      <td>0.032247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>29.977190</td>\n",
       "      <td>0.033359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mediocre best</th>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>27.909798</td>\n",
       "      <td>0.035830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unacceptable</th>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>27.909798</td>\n",
       "      <td>0.035830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horrible experience</th>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>27.909798</td>\n",
       "      <td>0.035830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waste money</th>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>27.909798</td>\n",
       "      <td>0.035830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inedible</th>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>27.909798</td>\n",
       "      <td>0.035830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horrible</th>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>25.497840</td>\n",
       "      <td>0.039219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horrible service</th>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>24.808709</td>\n",
       "      <td>0.040308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rudely</th>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>24.808709</td>\n",
       "      <td>0.040308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad restaurant</th>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>24.808709</td>\n",
       "      <td>0.040308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wo back</th>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>24.808709</td>\n",
       "      <td>0.040308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filthy</th>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>24.808709</td>\n",
       "      <td>0.040308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flavorless</th>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>24.808709</td>\n",
       "      <td>0.040308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ca even</th>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>21.707621</td>\n",
       "      <td>0.046067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unfriendly</th>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>21.707621</td>\n",
       "      <td>0.046067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          bad      good  bad_ratio  good_ratio\n",
       "word                                                          \n",
       "never return         0.000239  0.000006  37.213064    0.026872\n",
       "service horrible     0.000239  0.000006  37.213064    0.026872\n",
       "poor service         0.000239  0.000006  37.213064    0.026872\n",
       "never come           0.000219  0.000006  34.111975    0.029315\n",
       "unprofessional       0.000199  0.000006  31.010886    0.032247\n",
       "disgust              0.000578  0.000019  29.977190    0.033359\n",
       "mediocre best        0.000179  0.000006  27.909798    0.035830\n",
       "unacceptable         0.000179  0.000006  27.909798    0.035830\n",
       "horrible experience  0.000179  0.000006  27.909798    0.035830\n",
       "waste money          0.000179  0.000006  27.909798    0.035830\n",
       "inedible             0.000179  0.000006  27.909798    0.035830\n",
       "horrible             0.001475  0.000058  25.497840    0.039219\n",
       "horrible service     0.000160  0.000006  24.808709    0.040308\n",
       "rudely               0.000160  0.000006  24.808709    0.040308\n",
       "bad restaurant       0.000160  0.000006  24.808709    0.040308\n",
       "wo back              0.000160  0.000006  24.808709    0.040308\n",
       "filthy               0.000160  0.000006  24.808709    0.040308\n",
       "flavorless           0.000160  0.000006  24.808709    0.040308\n",
       "ca even              0.000279  0.000013  21.707621    0.046067\n",
       "unfriendly           0.000140  0.000006  21.707621    0.046067"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.sort_values(by='bad_ratio', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I love Dr. Scott!! He is the funniest doctor I've ever met and he makes me feel good too! Dr. Scott is always willing to bend over backwards to make sure you are provided the best service and treatment possible. The massage therapists rock too! I will be a lifelong patient of Integrated Chiropractic.\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp[yelp.processed_text.str.contains('dr ')].iloc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WOW this place is good!  SO good!  And not just yummy good, but intrinsically good.  Check out their amazing list of environmentally responsible business practices - http://essencebakery.com/essence_bakery_environmentally_friendly.shtml !  \n",
      "\n",
      "That, and it's cute.  And it tastes Yummy!  And they are SO nice!  SOOO nice!  And their deserts are just disgustingly cute and beautiful and freaking good.  I almost bought the mini box of 4 cupcakes for just $3.50.  Bite sized so not too bad, but if I bought them today they'd be gone before I got home. \n",
      "\n",
      "I got their grilled cheese w/ mozzarella, basil and tomato on grilled buttery brioche bread with a light and tasty green salad on the side.  It will be hard to not come here every week.  I love that all the drinks are refillable - and it's up to you to refill them.  Coffee, tea or soda, just walk on up and fill your glass.  And I love that it's all so fresh and local!  \n",
      "\n",
      "Important note - You order at the counter, they give you a number and bring out your food.  If you're paying w/ debit card, they will ask you if you want to include a tip.  That threw me.  I'd been in the place less than 2 minutes - how do I know if it's worth the tip - or how much of a tip it's worth??  Bring some cash or plan to deal w/ that question accordingly.\n"
     ]
    }
   ],
   "source": [
    "print(yelp[yelp.processed_text.str.contains('mozzarella')].iloc[3].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
